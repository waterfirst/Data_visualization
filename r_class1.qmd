---
title: "R을 이용한 데이터 전처리와 시각화 기초 코스"
author: "waterfirst"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    link-external-filter: '^(?:http:|https:)\/\/www\.quarto\.org\/custom'
editor: visual
code-fold: true
Rendering:
  embed-resources: true
execute:
  message : false
  warning : false
  error : false
  echo : true
lightbox: true
---

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/1200px-R_logo.svg.png)

## 1 Introduction

이 수업은 코딩을 전혀 모르는 사람들을 대상으로 숫자로 된 데이터를 적절히 칼질하여 요리할 수 있도록 하는 것을 목적으로 만들었습니다.

반복적으로 정형화된 데이터를 처리하고 그래프를 그리는 연구원들은 최종적으로는 자신의 결과물을 알기 쉽게 표현하는 것입니다.

이를 위해 tidyverse 패키지 하나만으로 얼마나 쉽게 데이터를 다룰 수 있는지 R의 장점이 무엇인지 알 수 있는 시간이 될 것입니다.

**R 언어 간단 소개**

두명의 뉴질랜드 통계학자가 만듦 : 로버트 젠틀맨(Robert Gentleman)과 로스 이하카(Ross Ihaka)

해들리 위컴에 의해 빅데이터 툴로 발전함 (대표적 : ggplot, tidyverse) <br/>

![](https://149357281.v2.pressablecdn.com/wp-content/uploads/2017/09/9.28-1.png)

**언어의 특징**

1부터 시작 (다른 언어들은 0부터 시작)

**패키지 설치, 불러오기**

-   install.packages(“패키지이름”)

-   library(패키지이름)

### 프로그램 구분

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHzmVo%2FbtqYo0P6SWZ%2FtDiMUtNpC8VETbYmmx7nW1%2Fimg.png)

Back end를 담당하는 데이터 전처리 및 시각화는 tidyverse 패키지를 이용하여 진행하고 필요할 경우, 추가 패키지를 이용할 것입니다.

실전에서 바로 쓸 수 있도록 기본 예제 데이터를 이용하여 학습하고 각자 자신의 자주 사용하는 데이터를 이용하여 반복 적으로 하던 일을 코딩을 통해 줄이고 더 창의적인 일에 시간을 쓸 수 있도록 4주 과정으로 만들었습니다. (주1회 2\~3시간)

<br/>

## 2 강의순서

1.  R 설치, 기본문법 (1주차)

    <https://dplyr.tidyverse.org/articles/dplyr.html>

2.  데이터 전처리 문제 풀이 (2주차)

    <https://m-clark.github.io/data-processing-and-visualization/intro.html>

3.  데이터 전처리 및 시각화 (3주차)

    <https://r-graph-gallery.com/>

4.  다양한 데이터 시각화 연습 (4주차)

    2d, 3d 이미지화

    -   ![](images/logo1.png)

    -   html로 문서 만들기

        <https://waterfirst.github.io/LENS_EXPERIMENT/>

------------------------------------------------------------------------

## 3 강의전 사전 준비

(#1\~3까지 하고, #4\~7은 나중에\~\~)

1.  R 설치 : <https://posit.co/download/rstudio-desktop/>

2.  RStudio 설치 <https://posit.co/download/rstudio-desktop/>

3.  Quarto CLI설치 : <https://quarto.org/docs/download/>

4.  Latex 설치 : (Rstudio 터미널창) **\$ quarto install tinytex**

5.  출판용 사이트 가입 : <https://quartopub.com/>

6.  github 가입 : <https://github.com/>

7.  git 설치 : <https://git-scm.com/download/win>

\[Quarto \]<https://quarto.org/docs/presentations/revealjs/>

프로그램을 배울 때, 다운로드, 설치, 환경설정만 하면 50%는 이미 배운것입니다. \^\^

### RStudio 설명

![](./images/rstudio_window.png)

## 4 Day1

-   데이터 분석과 시각화를 하는데 R이 최선인가?

ex) 상용 프로그램 : 엑셀 , 미니탭, 오리진, 매트랩, 스팟파이어

오픈소스 : 파이썬, R

-   왜 데이터 분석 및 시각화가 필요한가? GPT 시대인데...

-   내가 하고 있는 분야에 데이터는 정형화된 데이터인가?(숫자) 아니면 비정형 데이터인가(문자)

-   데이터 분석의 최종 목적은 무엇인가?

------------------------------------------------------------------------

### 4-1. R Basic

[1. 데이터 형식]{style="color:red;"}

```         
숫자형(numeric) : num(숫자형), int(정수형), dbl(실수형)
문자형(character) : chr
범주형(factor) : fct
논리형(logical) : logi
결측 (Not Available) : NA
무한대 (Infinite) : Inf
데이터 형식 알아보기 : class(변수명) is.numeric(변수명), is.character(변수명), is.factor(변수명)
데이터 형식 바꾸기 : as.numeric(변수명), as.factor(변수명), as.character(변수명), as.logical(변수명)
```

::: callout-note
범주형 변수(factor) : 그래프를 그리거나 통계적 분석시 유용함

데이터를 `열별`로 모아 놓은 `dataframe`, `tibble` 이 실제 분석에 이용

`list`, `matrix`, `array` 형태도 있음
:::

::: callout-tip
#### 단축키

`<-` : Alt + -

`실행` : Ctrl + enter

`|>` : Ctrl + Shift + M

`주석처리` : Ctrl + Shift + C
:::

<br/>

[2. 자주 사용 하는 함수]{style="color:red;"}

```         
평균(mean) : mean(변수)
중위수(median) : median(변수)
최대값(max) : max(변수)
최소값(min) : min(변수)
합(sum) : sum(변수)
표준편차(sd) : sd(변수)
분산(var) : var(변수)
절대값(abs) : abs(변수)
반올림(round) : round(변수, 반올림할 소수점 아래수)
제곱근(sqrt) : sqrt(변수)
원소갯수, 문자열길이(length) : length(변수)
행, 열의 수(dim) : dim(df)
프린트(print) : print(변수) / print(“문자”)
조건(ifelse) : ifelse(x>10, “a”, “b”)
중복없이 관측치 종류(unique) : unique(변수)
문자패턴 찾기(grep, grepl) : grep(“문자”, df):열번호 출력, grepl(“문자”, df):true/false로 출력
문자패턴 찾아 바꾸기(gsub) : gsub(“이전문자”, “새로운 문자”, df)
열갯수(ncol) : ncol(df)
행갯수(nrow) : nrow(df)
열이름(colnames) : colnames(df)
행이름(colnames) : rownames(df)
빈도수 구하기(table) : table(변수)
정렬하기(sort) : 내림차순 sort(변수), 오름차순 sort(변수, decreasing = TRUE)
열이름(names, colnames) : names(변수)
최대, 최소위치 찾기(which.max, which.min) : which.max(변수), which.min(변수)
```

::: callout-tip
### 4-2. 데이터 탐색 기본 함수

`head` : 앞 6개 행 보기

`tail` : 뒤 6개 행 보기

`summary` : 기술 통계 간단히 보기

`str` : 데이터 형식 보기
:::

<br/>

[3. 연산 기호]{style="color:red;"}

```         
"
* (곱하기) : x*2
/ (나누기) : x/2
%/% (나눗셈의 몫) : 16%/%3 = 5
%% (나눗셈의 나머지) : 16%/%3 = 1
== (일치, True or False) : 3==5, False
!= (불일치) : 3!=5, True
& (and) : x > 2 & x < 10
| (or) : x < 2 | x > 10
"
```

<br/>

### 4-3. Tidyverse

\[참고 자료\]<https://rstudio.github.io/cheatsheets/html/data-transformation.html>

```         
%>% (파이프라인, 왼쪽 데이터프레임을 오른쪽 함수에 넣어라) : df %>% head()

filter (조건에 맞는 행 추출) : df %>% filter(컬럼명 == “a”)

select(특정열 선택) : df %>% select(열번호) / df[, 열번호]

slice(특정행 선택) : df %>% slice(행번호) / df[행번호, ]
mutate(특정열 추가) : df %>% mutate(새로운 열이름 = )
rename(열이름 바꾸기) : df %>% rename(새로운 열이름 = 이전 열이름)
arrange(정렬하기) : 오름차순 : df %>% arrange(열이름), 내림차순 : df %>% arrange(desc(열이름))

group_by(특정열 그룹화), summarise(통계치 계산) :

df %>% group_by(열이름) %>% summarise(평균=mean(열이름))
열합치기(inner_join, full_join, left_join, right_join) : inner_join(df1, df2, by=“name”)

separate(특정기호로 분리) : df %>% separate(열이름, into = c("a", "b"), sep = "_")

na가 있는 행 제거하기(na.omit) : na.omit(df)

na가 있는 열에서 na 는 제거하고 계산하기 (na.rm=T) : mean(df, na.rm=T)

열합치기(cbind, bind_cols) : cbind(df1, df2) or bind_cols(df1, df2)
행합치기(rbind, bind_rows) : rbind(df1, df2) or bind_rows(df1, df2)

중복없는 값 찾기(distinct) : df %>% distinct ("열이름")

행의 수 세기 : n(), count()
```

### 4-4. long_form, wide_form

![](https://www.statology.org/wp-content/uploads/2021/12/wideLong1-1.png)

iris data를 이용하여 꽃잎 길이, 넓이, 꽃받임 길이, 넓이를 long form으로 바꾸어보자.

```{r}
library(tidyverse)
head(iris)

iris |> pivot_longer(cols = Sepal.Length:Petal.Width, names_to = "measure", values_to = "value") |> head()

iris |> pivot_longer(cols = Sepal.Length:Petal.Width, 
                     names_to = c("name1", "name2"),
                     names_sep ='\\.') |> head()

iris_long <- 
  iris |> pivot_longer(cols = Sepal.Length:Petal.Width, names_to = "measure", values_to = "value")


iris_long |> pivot_wider(
    names_from = measure,  values_from = value) |> unnest() |> head()
```

<br/>

### 4-4. 연습문제

![](images/penguin.png)

-   palmer penguin을 df에 넣고 앞 6개 행을 살펴보라.

    ```{r}
    #install.packages("palmerpenguins")
    library(palmerpenguins)
    df <-  penguins
    head(df)
    ```

-   데이터 탐색을 하라 (EDA : str, summary 이용)

    ```{r}
    str(df)
    summary(df)
    ```

-   NA가 있는 열 확인하라

    ```{r}
    colSums(is.na(df))
    ```

-   컬럼명에서 \_mm 제거하고 6개 행 보기(rename 이용 )

    ```{r}
    library(tidyverse)
    df |> rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      head()
      
    ```

-   Adelie 펭귄의 부리 길이 평균은 얼마일까?

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      filter(species =="Adelie")  |> 
      summarise("부리길이" = mean(bill_length, na.rm=T))
    ```

-   각 펭귄의 부리 길이, 부리 높이의 평균 구하라(소수 첫째자리까지 구하라)

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      group_by(species) %>% summarise("부리길이"=round(mean(bill_length, na.rm=T),1), "부리높이"=round(mean(bill_depth, na.rm=T),1))
    ```

-   펭귄 종류별 몇마리인가

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      group_by(species) %>%
      summarise(n=n())
    ```

-   펭귄종류, 부리길이, 부리높이 열만 선택해서 보여줘라 (6개 행)

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      select(species, bill_length, bill_depth) %>% head()
    ```

-   10행에서 15행을 보여주라.

    ```{r}
    df %>% slice(10:15)
    ```

-   새로운 변수를 만들어라 (bill_ratio = bill_lenght/bill_depth) : mutate

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      mutate(bill_ratio=bill_length/bill_depth) |> 
      head()
    ```

-   위 문제에서 NA 가 있는 행은 제거하고 보여줘라

    ```{r}
    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      mutate(bill_ratio=bill_length/bill_depth) |> 
      na.omit() %>% head()
    ```

-   Adelie, Chinstrap 펭귄의 각각 body_mass가 가장 작은 10개의 평균 부리길이(bill_length)를 구해서 두 평균 차이를 계산하라

    ```{r}
    avg1 <- df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      filter(species=="Adelie") |> 
      arrange(body_mass_g) |> 
      slice(1:10) |> 
      summarise(bl=mean(bill_length))
    avg2 <- df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      filter(species=="Chinstrap") |> 
      arrange(body_mass_g) |> 
      slice(1:10) |> 
      summarise(bl=mean(bill_length))

    result<- abs(avg1$bl-avg2$bl)
    print(result)
    ```

-   부리 길이(bill_length) 중 최빈값(가장 많은 수)을 찾아라.

    ```{r}

    df |> 
       rename(bill_length = bill_length_mm,
                 bill_depth = bill_depth_mm,
                 flipper_length = flipper_length_mm) |>
      select(bill_length) |> 
      table()  -> y

    names(y)[which(y==max(y))] 

    ```

### 4-5. 숙제

::: callout-note
Data : gapminder 연도별, 나라별 기대수명, 인구수, 1인당 GDP

`library(gapminder)` 로 데이터 불러오기
:::

![](images/gapminder.png)

::: callout-tip
## 문제

1.  2007년 대륙별 나라수는 몇 나라인가?

2.  가장 최근 연도에서 인구수가 많은 상위 10개 나라를 뽑아서 나라별 인구수와 기대 수명을 구하라. (이때 인구수는 13.2억명, 기대수명은 73세로 단위를 맞추어라. ) )

3.  연도별 기대수명이 가장 빠르게 증가한 나라 10개를 순서대로 나열하시오. (1952년, 2007년 비교)

4.  2002년도 대륙별 1인당 gpd의 평균과 표준편차는 어떻게 되는가?

5.  기대수명 데이터를 표준화(평균 0, 표준편차 1) 하라.

6.  Kuwait 를 제외하고, 1인당 gpd 데이터를 정규화(1과 0 사이로 만듦) 하라
:::

::: callout-warning
## Hint

`정규화 함수` nor_minmax = function(x){ result = (x - min(x)) / (max(x) - min(x)) return(result) }

`표준화 함수` nor_sd = function(x){ result = (x - mean(x)) / sd(x) return(result) }
:::

![](images/nor_scale.png)

::: {.callout-caution collapse="true"}
## 정답(R code)

1.  gapminder \|\> filter(year == 2007) \|\> group_by(continent) \|\> summarise(n= n())
2.  gapminder \|\> filter(year == 2007) \|\> arrange(-pop) \|\> slice(1:10) \|\> group_by(country) \|\> summarise(인구수_억명 = round(pop/100000000,1), 기대수명_세 = round(lifeExp) ) \|\> arrange(-인구수_억명)
3.  gapminder \|\> select(country, year, lifeExp) \|\> filter(year %in% c(1952, 2007)) \|\> pivot_wider(names_from = year, values_from = lifeExp) \|\> mutate(ratio = (`2007`- `1952`)/(2007-1952)) \|\> arrange(-ratio)
4.  gapminder \|\> filter(year == 2002) \|\> group_by(continent) \|\> summarise(avg = mean(gdpPercap, na.rm=T), `σ`= sd(gdpPercap, na.rm=T))
5.  nor_sd = function(x){ result = (x - mean(x)) / sd(x) return(result) }

gapminder \|\> mutate(life_nor = nor_sd(lifeExp) )

6.  nor_minmax = function(x){ result = (x - min(x)) / (max(x) - min(x)) return(result) }

gapminder \|\>

filter(country != "Kuwait") \|\> mutate(gdp_sd = nor_minmax(gdpPercap) )
:::

------------------------------------------------------------------------

## 5. Day2 (전처리 연습문제)

### [5-1. 유튜브 데일리 인기동영상 분석]{style="color:blue;"}

*\[출처\] 10주차 예상문제 (실기1 유형) (이기적 스터디 카페)*

dataurl =https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv

-   패키지로드, 데이터 불러오기

```{r}
#| echo = T
library(tidyverse)

df<-read.csv("https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv")

```

------------------------------------------------------------------------

-   [Q1. 인기동영상 제작횟수가 많은 채널 상위 10개명을 출력하라 (날짜기준, 중복포함)]{style="color:red;"}

```{r}

#Q1

df%>%
  group_by(channelTitle)%>%
  summarise(n=n()) |> 
  arrange(-n) |> 
  slice(1:10)

```

------------------------------------------------------------------------

-   [Q2. 논란으로 인기동영상이 된 케이스를 확인하고 싶다. dislikes수가 like 수보다 높은 동영상을 제작한 채널을 모두 출력하라]{style="color:red;"}

```{r}

#Q2

df |> 
  filter(dislikes>likes) |> 
  select(channelTitle) |> 
  distinct()

```

------------------------------------------------------------------------

-   [Q3. 일요일에 인기있었던 영상들중 가장많은 영상 종류(categoryId)는 무엇인가?]{style="color:red;"}

```{r}
#Q3

df |> 
  mutate(요일 = wday(trending_date2, label=T)) |> 
  filter(요일 == "일") |> 
  filter(likes> dislikes) |> 
  group_by(categoryId) |> 
  summarise(n = n()) |> 
  arrange(-n) |> 
  select(1) |> 
  slice(1)

```

------------------------------------------------------------------------

-   [Q4. 각 요일별 인기 영상들의 categoryId는 각각 몇개 씩인지 하나의 데이터 프레임으로 표현하라]{style="color:red;"}

```{r}
#Q5

df |> 
  mutate(요일 = wday(trending_date2, label=T)) |> 
  select(categoryId, 요일) |> 
  table()

```

------------------------------------------------------------------------

-   [Q5. 댓글의 수로 (comment_count) 영상 반응에 대한 판단을 할 수 있다. viewcount대비 댓글수가 가장 높은 영상을 확인하라 (view_count값이 0인 경우는 제외한다)]{style="color:red;"}

```{r}
#Q5

df |> 
  filter(view_count != 0) |> 
  mutate(ratio = comment_count/view_count) |> 
  arrange(-ratio)|> 
  slice(1) |> 
  select(title )

```

------------------------------------------------------------------------

-   [Q6. like 대비 dislike의 수가 가장 적은 영상은 무엇인가? (like, dislike 값이 0인경우는 제외한다)]{style="color:red;"}

```{r}
#Q6

df |> 
  filter(dislikes != 0 & likes != 0) |> 
  mutate(n = likes / dislikes) |> 
  arrange(-n) |> 
  slice(1) |> 
  select(title)


```

------------------------------------------------------------------------

-   [Q7. 가장많은 트렌드 영상을 제작한 채널의 이름은 무엇인가? (날짜기준, 중복포함)]{style="color:red;"}

```{r}
#Q7


df |> 
  group_by(channelTitle)%>%
  summarise(n=n()) |> 
  arrange(-n) |> 
  slice(1)


```

------------------------------------------------------------------------

-   [Q8. 20회(20일)이상 인기동영상 리스트에 포함된 동영상의 숫자는?]{style="color:red;"}

```{r}
#Q8

df |> 
  group_by(title) |> 
  summarise(n = n()) |> 
  filter(n >= 20) |> 
  count()
  

```

<hr/>

<hr/>

### [5-2. 넷플릭스 데이터 분석]{style="color:blue;"}

*\[출처\] 9주차 예상문제 (실기1 유형) (이기적 스터디 카페)*

dataurl =https://raw.githubusercontent.com/Datamanim/datarepo/main/nflx/NFLX.csv

-   패키지로드, 데이터 불러오기

```{r}
#| echo = T
df <- read.csv("https://raw.githubusercontent.com/Datamanim/datarepo/main/nflx/NFLX.csv")

```

-   [Q1. 매년 5월달의 open 가격의 평균값을 데이터프레임으로 표현하라.]{style="color:red;"}

```{r}
#Q1

library(lubridate)

df |> mutate(year = year(Date),
             month = month(Date)) |>
  filter(month == 5) |> 
  group_by(year) |> 
  summarise(평균 = mean(Open))
  
  
```

<hr/>

<hr/>

### [5-3. 날짜 다루기]{style="color:blue;"}

To learn more about **lubridate** see <https://lubridate.tidyverse.org/>.

-   패키지 설치, 불러오기

```{r}
#| echo = T

#install.packages('lubridate')
library('lubridate')
```

-   문자로 표현된 날짜를 날짜변수로 바꾸기

```{r}
#| echo = T

date <- '2020-01-10'
class(date)
date2 <- as.Date(date)
class(date2)

```

-   연, 월, 일 뽑아내기

```{r}
#| echo = T

year(date)
month(date)
day(date)
ymd(date)

```

-   주, 요일 뽑아내기

```{r}
#| echo = T
week(date)
wday(date)
wday(date, label = T)

```

-   시간, 분, 초 뽑아내기

```{r}
#| echo = T
now()
time <- now()
hour(time)
minute(time)
second(time)
ymd_hms(time)
```

------------------------------------------------------------------------

## 

### [5-4. 강수량 분석]{style="color:blue;"}

*\[출처\] 1주차 예상문제 (실기1 유형) (이기적 스터디 카페)*

dataurl = https://raw.githubusercontent.com/Datamanim/datarepo/main/weather/weather2.csv

-   패키지로드, 데이터 불러오기

```{r}
#| echo = T
library(tidyverse)

df<-read.csv("https://raw.githubusercontent.com/Datamanim/datarepo/main/weather/weather2.csv")

```

------------------------------------------------------------------------

-   [Q1. 여름철(6월,7월,8월) 이화동이 수영동보다 높은 기온을 가진 시간대는 몇개인가?]{style="color:red;"}

```{r}

#Q1

library(lubridate)

df |> 
  mutate(월 = month(time),
         시간 = hour(time)) |> 
  filter(월 %in% c(6,7,8),
         이화동기온 > 수영동기온) |> 
  nrow()

```

-   [Q2. 이화동과 수영동의 최대강수량의 시간대를 각각 구하여라]{style="color:red;"}

```{r}

#Q2

df |> 
  filter(이화동강수 == max(이화동강수 ) ) |> 
  select(time)

df |> 
  filter(수영동강수 == max(수영동강수)) |> 
  select(time)

```

## 6. Day3 (데이터 불러와서 전처리, 시각화)

To learn more about **tidyr** see <https://tidyr.tidyverse.org/reference/pivot_longer.html/>.

데이터 분석의 첫 걸음은 데이터를 불러오는 과정이다.

1.  R의 내장 데이터에서 불러오기

    data() , help("AirPassengers")

<https://vincentarelbundock.github.io/Rdatasets/datasets.html>

```{r}

data(AirPassengers)
AirPassengers

plot(AirPassengers, main = "Airline Passengers Over Time",
     xlab = "Year-Month", ylab = "Number of Passengers")

```

2.  외장데이터 불러오기 (package 설치, library로 불러오기)

    gapminder : 세계 여러 국가의 인구, 경제, 건강 등의 데이터를 포함

```{r}

#install.packages("gapminder")
library(gapminder)

data(gapminder)
head(gapminder)
```

3.  클릭보드(엑셀)에서 붙여넣기

    datapaste 패키지 설치 -\> 엑셀에서 ctrl+c -\> RStudio의 Addins에서 Paste as tribble

<!-- -->

4.  csv 파일에서 불러오기

```{r}

#| eval: true

 #  read.csv ("D:/r/data/test.csv")       ## **/** 방향 주의
 #  read.csv ("D:\\r\\data\\test.csv")    ## **\\** 방향 주의
   
```

5.  엑셀파일 불러오기 <https://readxl.tidyverse.org/>

```{r}

#| eval: true

#   install.packages('readxl')
#   library(readxl)
#   read_excel("my_file.xls")
   
```

6.  구글시트에서 불러오기

\[참고\] <https://googlesheets4.tidyverse.org/>

```{r}

#install.packages("googlesheets4")
library(googlesheets4)
gs4_deauth()

df <- read_sheet("https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w/edit?gid=0#gid=0")

head(df)
```

7.  NA 처리하기 <https://tidyr.tidyverse.org/reference/fill.html>

```{r}
sales <- tibble::tribble(
  ~quarter, ~year, ~sales,
  "Q1",    2000,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",      NA,    21001,
  "Q1",    2001,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",      NA,    50197,
  "Q1",    2002,    39113,
  "Q2",      NA,    41668,
  "Q3",      NA,    30144,
  "Q4",      NA,    52897,
  "Q1",    2004,    32129,
  "Q2",      NA,    67686,
  "Q3",      NA,    31768,
  "Q4",      NA,    49094
)


# `fill()` defaults to replacing missing data from top to bottom
sales %>% fill(year, .direction = "down")
```

8.  NA를 평균, 중앙값으로 대체하기

```{r}
head(airquality)
colSums(is.na(airquality))

airquality |> 
  mutate(Ozone = ifelse(is.na(Ozone), mean(Ozone, na.rm=T), Ozone),
         Solar.R = ifelse(is.na(Ozone), median(Ozone, na.rm=T), Solar.R)) -> airquality2

colSums(is.na(airquality2))


```

### 6-1. table 만들기

-   \[gt package\]<https://gt.rstudio.com/articles/gt.html>

1)  간단한 테이블 만들어보기 (나라별 섬 갯수)

```{r}
library(tidyverse)
library(gt)

df <- tibble(
    name = names(islands),
    size = islands
  ) 

df |> 
  arrange(-size)|>
  slice(1:10) |> 
  gt()
```

![](https://gt.rstudio.com/reference/figures/gt_parts_of_a_table.svg){width="719"} 2) Title, subtitle 넣기

```{r}

df |> 
  arrange(-size)|>
  slice(1:10) |> 
  gt() |> 
    tab_header(
    title = "Large Landmasses of the World",
    subtitle = "The top ten largest are presented"
  )
```

3)  제목 꾸미기 (마크다운 문법 md)

```{r}
df |> 
  arrange(-size)|>
  slice(1:2) |> 
  gt() |> 
  tab_header(
    title = md("**Large Landmasses of the World**"),
    subtitle = md("The *top two* largest are presented")
  )
```

4)  바닥글에 출처 넣기(tab_source_note)

```{r}
df |> 
  arrange(-size)|>
  slice(1:10) |> 
  gt() |> 
  tab_header(
    title = md("**Large Landmasses of the World**"),
    subtitle = md("The *top two* largest are presented")
  ) |> 
    tab_source_note(
    source_note = "Source: The World Almanac and Book of Facts, 1975, page 406."
  ) |>
  tab_source_note(
    source_note = md("Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.")
  )
```

5)  주석 넣기(tab_footnote)

```{r}
df |> 
  arrange(-size)|>
  slice(1:10) |> 
  gt() |> 
  tab_header(
    title = md("**Large Landmasses of the World**"),
    subtitle = md("The *top two* largest are presented")
  ) |> 
    tab_source_note(
    source_note = "Source: The World Almanac and Book of Facts, 1975, page 406."
  ) |>
  tab_source_note(
    source_note = md("Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.")
  ) |> 
  
    tab_footnote(
    footnote = "The Americas.",
    locations = cells_body(columns = name, rows = 3:4)
  ) |> 
  
  tab_footnote(
    footnote = "The largest by area.",
    locations = cells_body(
      columns = size,
      rows = size == max(size)
    )
  ) |>
  tab_footnote(
    footnote = "The lowest by area.",
    locations = cells_body(
      columns = size,
      rows = size == min(size)
    )
  )
```

6)  table 저장하기

```{r}
df |> 
  arrange(-size)|>
  slice(1:10) |> 
  gt() |> 
  tab_header(
    title = md("**Large Landmasses of the World**"),
    subtitle = md("The *top two* largest are presented")
  ) |> 
    tab_source_note(
    source_note = "Source: The World Almanac and Book of Facts, 1975, page 406."
  ) |>
  tab_source_note(
    source_note = md("Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.")
  ) |> 
  
    tab_footnote(
    footnote = "The Americas.",
    locations = cells_body(columns = name, rows = 3:4)
  ) |> 
  
  tab_footnote(
    footnote = "The largest by area.",
    locations = cells_body(
      columns = size,
      rows = size == max(size)
    )
  ) |>
  tab_footnote(
    footnote = "The lowest by area.",
    locations = cells_body(
      columns = size,
      rows = size == min(size)
    )
  ) |> 
  #  gtsave(filename = "tab_1.html") |> 
  gtsave("tab_1.png", expand = 10)
```

### 6-2. 시각화 하기

\[참고 자료\]<https://waterfirst.quarto.pub/r_course/#/title-slide> \[참고 자료\]<https://rstudio.github.io/cheatsheets/html/data-visualization.html>

ChickWeight 데이터셋:

weight: 닭의 체중 Time: 실험 시간 Chick: 닭의 고유 식별자 Diet: 실험 그룹을 나타내는 범주형 변수로, 각 닭이 어떤 종류의 식사를 받았는지를 나타냅니다.

```{r}
head(ChickWeight)

ChickWeight |> ggplot(aes(Time, weight, col=Chick, fill=Diet))+geom_point()+
  geom_line()+
  facet_wrap(~Diet,)+
  theme(legend.position = "none")
```

#### 산점도 그래프

\[참고자료\]<https://ggplot2.tidyverse.org/index.html>

```{r}
library(tidyverse)

ggplot(mpg, aes(cty, hwy)) +
  geom_point(mapping = aes(colour = displ)) +
  geom_smooth(formula = y ~ x, method = "lm") +
  scale_colour_viridis_c() +
  facet_grid(year ~ drv) +
  coord_fixed() +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())


```

#### Color 팔레트

```{r}
library(RColorBrewer)
display.brewer.all()
```

사용법 :

scale_fill_brewer(palette="Set1")

scale_colour_brewer(palette="Set1")

\[Color Pick Up\](<https://r-graph-gallery.com/ggplot2-color.html>)

\[Colorspace 패키지\](<https://m.blog.naver.com/regenesis90/222234511150>)

\[Sci-Fi\](<https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html>)

#### 막대 그래프

\[참고자료\]<https://r-charts.com/ranking/bar-plot-ggplot2/>

```{r}
# install.packages("ggplot2")
library(ggplot2)
df <- data.frame(group = c("A", "B", "C"),
                 count = c(3, 5, 6))
ggplot(df, aes(x = group, y = count, fill=group)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), vjust = -1, colour = "black") +
  ylim(c(0, 6.5)) # Increase the limits of the Y-axis if needed 
  
```

```{r}

# install.packages("ggplot2")
library(ggplot2)

ggplot(df, aes(x = group, y = count, fill=group)) +
  geom_col() 

```

#### Boxplot 그래프

\[참고자료\]<https://ggplot2.tidyverse.org/reference/geom_boxplot.html>

```{r}


ggplot(mpg, aes(class, hwy)) + geom_boxplot(aes(colour = drv))

```

#### viloin 그래프

\[참고자료\]<https://r-charts.com/es/distribucion/grafico-violin-grupo-ggplot2/>

```{r}


# install.packages("ggplot2")
library(ggplot2)

ggplot(warpbreaks, aes(x = tension, y = breaks, fill = tension)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.07) 


```

#### Density 그래프

\[참고자료\]<https://r-charts.com/es/distribucion/grafico-densidad-grupo-ggplot2/>

```{r}

# Datos
set.seed(5)
x <- c(rnorm(200, mean = -2, 1.5),
       rnorm(200, mean = 0, sd = 1),
       rnorm(200, mean = 2, 1.5))
grupo <- c(rep("A", 200), rep("B", 200), rep("C", 200))
df <- data.frame(x, grupo)

# install.packages("ggplot2")
library(ggplot2)

cols <- c("#F76D5E", "#FFFFBF", "#72D8FF")

# Gráfico de densidad en ggplot2
ggplot(df, aes(x = x, fill = grupo)) +
  geom_density(alpha = 0.7) + 
  scale_fill_manual(values = cols) 
 

```

#### Pair 그래프

\[참고자료\]<https://r-charts.com/es/correlacion/ggpairs/>

```{r}

# install.packages("GGally")
library(GGally)

ggpairs(iris)  

# install.packages("GGally")
library(GGally)

ggpairs(iris, columns = 1:4, aes(color = Species, alpha = 0.5),
        upper = list(continuous = "points")) 


```

#### Sankey 그래프

\[참고자료\]<https://r-charts.com/es/flujo/diagrama-sankey-ggplot2/>

```{r}

# install.packages("remotes")
# remotes::install_github("davidsjoberg/ggsankey")

library(ggsankey)
df <- mtcars %>%
  make_long(cyl, vs, am, gear, carb) 

# install.packages("remotes")
# remotes::install_github("davidsjoberg/ggsankey")
library(ggsankey)
# install.packages("ggplot2")
library(ggplot2)
# install.packages("dplyr")
library(dplyr) # Necesario

ggplot(df, aes(x = x, 
               next_x = next_x, 
               node = node, 
               next_node = next_node,
               fill = factor(node),
               label = node)) +
  geom_sankey(flow.alpha = 0.5, node.color = 1) +
  geom_sankey_label(size = 3.5, color = 1, fill = "white") +
  scale_fill_viridis_d() +
  theme_sankey(base_size = 16) +
  theme(legend.position = "none") 

```

#### 그래프 분할하기

-   facet_grid

```{r}
#create data frame
df <- data.frame(team=c('A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'),
                 position=c('G', 'G', 'F', 'F', 'G', 'G', 'G', 'G'),
                 points=c(8, 14, 20, 22, 25, 29, 30, 31),
                 assists=c(10, 5, 5, 3, 8, 6, 9, 12))

ggplot(df, aes(assists, points)) +
  geom_point() +
  facet_grid(position~team)

```

-   facet_warp

```{r}
ggplot(df, aes(assists, points)) +
  geom_point() +
  facet_wrap(position~team)
```

#### Patchwork 패키지

```{r}
library(patchwork)

p1 <- ggplot(mtcars) + geom_point(aes(mpg, disp))
p2 <- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))

p1 + p2
```

```{r}
p3 <- ggplot(mtcars) + geom_smooth(aes(disp, qsec))
p4 <- ggplot(mtcars) + geom_bar(aes(carb))

(p1 | p2 | p3) /
      p4
```

#### 논문용 Theme

```{r}
theme_Publication <- function(base_size=14, base_family="helvetica") {
  library(grid)
  library(ggthemes)
  (theme_foundation(base_size=base_size, base_family=base_family)
    + theme(plot.title = element_text(face = "bold",
                                      size = rel(1.2), hjust = 0.5),
            text = element_text(),
            panel.background = element_rect(colour = NA),
            plot.background = element_rect(colour = NA),
            panel.border = element_rect(colour = NA),
            axis.title = element_text(face = "bold",size = rel(1)),
            axis.title.y = element_text(angle=90,vjust =2),
            axis.title.x = element_text(vjust = -0.2),
            axis.text = element_text(), 
            axis.line = element_line(colour="black"),
            axis.ticks = element_line(),
            panel.grid.major = element_line(colour="#f0f0f0"),
            panel.grid.minor = element_blank(),
            legend.key = element_rect(colour = NA),
            legend.position = "bottom",
            legend.direction = "horizontal",
            legend.key.size= unit(0.2, "cm"),
            legend.margin = unit(0, "cm"),
            legend.title = element_text(face="italic"),
            plot.margin=unit(c(10,5,5,5),"mm"),
            strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
            strip.text = element_text(face="bold")
    ))
  
}

scale_fill_Publication <- function(...){
  library(scales)
  discrete_scale("fill","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)
  
}

scale_colour_Publication <- function(...){
  library(scales)
  discrete_scale("colour","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)
  
}

```

#### Plotly 그래프

```{r}

library(ggplot2)
library(ggrepel)
library(dplyr)


temp.dat <- structure(list(Year = c("2003", "2004", "2005", "2006", "2007", 
                                    "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2003", 
                                    "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", 
                                    "2012", "2013", "2014", "2003", "2004", "2005", "2006", "2007", 
                                    "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2003", 
                                    "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", 
                                    "2012", "2013", "2014"), State = structure(c(1L, 1L, 1L, 1L, 
                                                                                 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
                                                                                 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
                                                                                 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c("VIC", 
                                                                                                                                             "NSW", "QLD", "WA"), class = "factor"), Capex = c(5.35641472365348, 
                                                                                                                                                                                               5.76523240652641, 5.24727577535625, 5.57988239709746, 5.14246402568366, 
                                                                                                                                                                                               4.96786288162828, 5.493190785287, 6.08500616799372, 6.5092228474591, 
                                                                                                                                                                                               7.03813541623157, 8.34736513875897, 9.04992300432169, 7.15830329914056, 
                                                                                                                                                                                               7.21247045701994, 7.81373928617117, 7.76610217197542, 7.9744994967006, 
                                                                                                                                                                                               7.93734452080786, 8.29289899132255, 7.85222269563982, 8.12683746325074, 
                                                                                                                                                                                               8.61903784301649, 9.7904327253813, 9.75021175267288, 8.2950673974226, 
                                                                                                                                                                                               6.6272705639724, 6.50170524635367, 6.15609626379471, 6.43799637295979, 
                                                                                                                                                                                               6.9869551384028, 8.36305663640294, 8.31382617231745, 8.65409824343971, 
                                                                                                                                                                                               9.70529678167458, 11.3102788081848, 11.8696420977237, 6.77937303542605, 
                                                                                                                                                                                               5.51242844820827, 5.35789621712839, 4.38699327451101, 4.4925792218211, 
                                                                                                                                                                                               4.29934654081527, 4.54639175257732, 4.70040615159951, 5.04056109514957, 
                                                                                                                                                                                               5.49921208937735, 5.96590909090909, 6.18700407463007)), class = "data.frame", row.names = c(NA, 
                                                                                                                                                                                                                                                                                           -48L), .Names = c("Year", "State", "Capex"))


head(temp.dat)
library(ggplot2)
library(ggrepel)
library(dplyr)


p <- temp.dat %>%
  mutate(label = if_else(Year == max(Year), as.character(State), NA_character_)) %>%
  ggplot(aes(x = Year, y = Capex, group = State, colour = State, shape=State)) + 
  geom_line() + geom_point()+
  geom_label_repel(aes(label = label),
                   nudge_x = 1,
                   na.rm = TRUE)+
  scale_colour_Publication()+ theme_Publication()+
  theme(legend.position = "none")
p

library(plotly)
ggplotly(p)
```

#### 그래프 저장

```{r}
p
ggsave("myplot.png")

p2 <- ggplotly(p)
htmlwidgets::saveWidget(p2, "myplot.html")
```

## 6. Day5

### 1. 지도그리기

\[datatoys\]<https://github.com/statgarten/datatoys>

데이터토이에서 맛집 (datatoys::restaurant) 을 이용하여 지도에 표시하기

leaflet 패키지 이용하기 (<https://bigdata-anlysis.tistory.com/34>)

```{r}
library(datatoys)
library(tidyverse)

library(gt)
library(leaflet)


set.seed(1234)

df <- datatoys::restaurant 
gt(df |> head())
colnames(df)
df %>% rename(lat = WGS84위도, lng = WGS84경도) %>% 
  select(lng, lat, 음식점명) -> m

leaflet() %>%  
  addTiles() %>% 
  addMarkers(lng = as.numeric(m$lng), lat = as.numeric(m$lat), popup=m$음식점명)

```

### 2. 머신러닝

![참조 : https://star7sss.tistory.com/410](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FS3MxJ%2FbtryQ1SaveU%2F9JsPZ0BPGJG1ubb4Tds8q0%2Fimg.png)

![](images/machine_learning_1.png)

### 2.1 지도학습 \[회귀 분석\]

R에서 회귀 분석을 위해 사용할 수 있는 주요 패키지들은 다음과 같습니다:

-   `stats`: R의 기본 패키지로, 선형 회귀를 위한 `lm()` 함수를 제공합니다.

-   `glmnet`: 정규화된 일반화 선형 모델을 위한 패키지입니다.

-   `randomForest`: 랜덤 포레스트 알고리즘을 구현한 패키지입니다.

-   `rpart`: 결정 트리 알고리즘을 구현한 패키지입니다.

-   `e1071`: 서포트 벡터 머신(SVM)을 포함한 여러 기계학습 알고리즘을 제공합니다.

-   `class`: k-최근접 이웃(KNN) 알고리즘을 위한 패키지입니다.

1.  선형 회귀 분석

```{r}
# 선형 회귀 예제
# 필요한 패키지 로드
library(ggplot2)

# 데이터 생성
set.seed(123)
n <- 100
x <- runif(n, 0, 10)
y <- 2 + 3 * x + rnorm(n, 0, 1.5)
data <- data.frame(x = x, y = y)

# 선형 회귀 모델 생성
model <- lm(y ~ x, data = data)

# 모델 요약
summary(model)

# 시각화
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal() +
  labs(title = "선형 회귀 예제",
       x = "독립 변수",
       y = "종속 변수")

# 예측
new_data <- data.frame(x = c(5, 7, 9))
predictions <- predict(model, newdata = new_data)
print(predictions)

# 설명:
# 1. 먼저 필요한 패키지(ggplot2)를 로드합니다.
# 2. 임의의 데이터를 생성합니다. x는 0에서 10 사이의 균일 분포, y는 x와 선형 관계를 가지며 약간의 노이즈를 추가합니다.
# 3. lm() 함수를 사용하여 선형 회귀 모델을 생성합니다.
# 4. summary() 함수로 모델의 상세 정보를 확인합니다.
# 5. ggplot2를 사용하여 데이터와 회귀선을 시각화합니다.
# 6. 새로운 x 값에 대한 y 값을 예측합니다.

# 이 예제를 통해 기본적인 선형 회귀 분석의 과정을 이해할 수 있습니다.
```

2.  랜덤포레스트 회귀 분석

```{r}
# 랜덤 포레스트 예제
# 필요한 패키지 로드
library(randomForest)
library(ggplot2)

# 데이터 준비 (Boston 주택 가격 데이터셋 사용)
data(Boston, package = "MASS")

# 데이터 분할 (훈련 세트와 테스트 세트)
set.seed(123)
train_index <- sample(1:nrow(Boston), 0.7 * nrow(Boston))
train_data <- Boston[train_index, ]
test_data <- Boston[-train_index, ]

# 랜덤 포레스트 모델 생성
rf_model <- randomForest(medv ~ ., data = train_data, ntree = 500, importance = TRUE)

# 모델 요약
print(rf_model)

# 변수 중요도 시각화
importance_df <- as.data.frame(importance(rf_model))
importance_df$feature <- rownames(importance_df)

ggplot(importance_df, aes(x = reorder(feature, IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "랜덤 포레스트 - 변수 중요도",
       x = "특성",
       y = "노드 불순도 감소")

# 테스트 세트에 대한 예측
predictions <- predict(rf_model, newdata = test_data)

# 모델 평가 (RMSE 계산)
rmse <- sqrt(mean((predictions - test_data$medv)^2))
print(paste("RMSE:", rmse))

# 설명:
# 1. randomForest 패키지를 로드합니다.
# 2. Boston 주택 가격 데이터셋을 사용합니다.
# 3. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
# 4. randomForest() 함수로 모델을 생성합니다. 종속 변수는 medv(주택 가격)입니다.
# 5. 모델 요약 정보를 출력합니다.
# 6. 변수 중요도를 시각화합니다.
# 7. 테스트 세트에 대한 예측을 수행하고 RMSE를 계산하여 모델 성능을 평가합니다.

# 이 예제를 통해 랜덤 포레스트를 사용한 회귀 분석의 과정을 이해할 수 있습니다.
```

3.  결정 트리 회귀 분석

```{r}
# 결정 트리 예제
# 필요한 패키지 로드
library(rpart)
library(rpart.plot)
library(ggplot2)

# 데이터 준비 (Boston 주택 가격 데이터셋 사용)
data(Boston, package = "MASS")

# 데이터 분할 (훈련 세트와 테스트 세트)
set.seed(123)
train_index <- sample(1:nrow(Boston), 0.7 * nrow(Boston))
train_data <- Boston[train_index, ]
test_data <- Boston[-train_index, ]

# 결정 트리 모델 생성
tree_model <- rpart(medv ~ ., data = train_data, method = "anova")

# 모델 요약
summary(tree_model)

# 결정 트리 시각화
rpart.plot(tree_model, box.palette = "RdBu", shadow.col = "gray", nn = TRUE)

# 변수 중요도 시각화
importance_df <- data.frame(
  feature = names(tree_model$variable.importance),
  importance = tree_model$variable.importance
)

ggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "결정 트리 - 변수 중요도",
       x = "특성",
       y = "중요도")

# 테스트 세트에 대한 예측
predictions <- predict(tree_model, newdata = test_data)

# 모델 평가 (RMSE 계산)
rmse <- sqrt(mean((predictions - test_data$medv)^2))
print(paste("RMSE:", rmse))

# 설명:
# 1. rpart와 rpart.plot 패키지를 로드합니다.
# 2. Boston 주택 가격 데이터셋을 사용합니다.
# 3. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
# 4. rpart() 함수로 결정 트리 모델을 생성합니다. method="anova"는 회귀 트리를 의미합니다.
# 5. 모델 요약 정보를 출력합니다.
# 6. rpart.plot() 함수로 결정 트리를 시각화합니다.
# 7. 변수 중요도를 시각화합니다.
# 8. 테스트 세트에 대한 예측을 수행하고 RMSE를 계산하여 모델 성능을 평가합니다.

# 이 예제를 통해 결정 트리를 사용한 회귀 분석의 과정을 이해할 수 있습니다.
```

### 2.2 지도학습 \[분류 분석\]

1.  `stats`: R의 기본 패키지로, 로지스틱 회귀를 위한 `glm()` 함수를 제공합니다.

2.  `randomForest`: 랜덤 포레스트 알고리즘을 구현한 패키지입니다.

3.  `rpart`: 결정 트리 알고리즘을 구현한 패키지입니다.

4.  `e1071`: 서포트 벡터 머신(SVM)을 포함한 여러 기계학습 알고리즘을 제공합니다.

5.  `class`: k-최근접 이웃(KNN) 알고리즘을 위한 패키지입니다.

6.  `caret`: 다양한 분류 알고리즘을 통합적으로 다룰 수 있는 패키지입니다.

<!-- -->

1.  로지스틱 회귀 (glm)

    ```{r}
    # 로지스틱 회귀 예제
    # 필요한 패키지 로드
    library(ggplot2)
    library(caret)

    # 데이터 준비 (iris 데이터셋 사용)
    data(iris)
    # 간단한 이진 분류를 위해 setosa와 versicolor만 사용
    iris_binary <- iris[iris$Species != "virginica", ]
    iris_binary$Species <- factor(iris_binary$Species)

    # 데이터 분할 (훈련 세트와 테스트 세트)
    set.seed(123)
    train_index <- createDataPartition(iris_binary$Species, p = 0.7, list = FALSE)
    train_data <- iris_binary[train_index, ]
    test_data <- iris_binary[-train_index, ]

    # 로지스틱 회귀 모델 생성
    model <- glm(Species ~ Petal.Length + Petal.Width, data = train_data, family = "binomial")

    # 모델 요약
    summary(model)

    # 결정 경계 시각화
    ggplot(train_data, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
      geom_point() +
      stat_function(fun = function(x) {-coef(model)[1]/coef(model)[3] - coef(model)[2]/coef(model)[3] * x}, 
                    geom = "line", color = "black") +
      theme_minimal() +
      labs(title = "로지스틱 회귀 - 결정 경계",
           x = "꽃잎 길이",
           y = "꽃잎 너비")

    # 테스트 세트에 대한 예측
    predictions <- predict(model, newdata = test_data, type = "response")
    predicted_classes <- ifelse(predictions > 0.5, "versicolor", "setosa")

    # 모델 평가 (정확도, 혼동 행렬)
    accuracy <- mean(predicted_classes == test_data$Species)
    conf_matrix <- table(Predicted = predicted_classes, Actual = test_data$Species)

    print(paste("정확도:", accuracy))
    print("혼동 행렬:")
    print(conf_matrix)

    # 설명:
    # 1. iris 데이터셋을 사용하여 이진 분류 문제를 설정합니다.
    # 2. caret 패키지의 createDataPartition() 함수로 데이터를 분할합니다.
    # 3. glm() 함수로 로지스틱 회귀 모델을 생성합니다. family="binomial"은 로지스틱 회귀를 의미합니다.
    # 4. 모델 요약 정보를 출력합니다.
    # 5. ggplot2를 사용하여 결정 경계를 시각화합니다.
    # 6. 테스트 세트에 대한 예측을 수행하고 정확도와 혼동 행렬을 계산하여 모델 성능을 평가합니다.

    # 이 예제를 통해 로지스틱 회귀를 사용한 이진 분류의 과정을 이해할 수 있습니다.
    ```

2.  랜덤포레스트 (randomforest)

    ```{r}
    # 랜덤 포레스트 분류 예제
    # 필요한 패키지 로드
    library(randomForest)
    library(ggplot2)
    library(caret)

    # 데이터 준비 (iris 데이터셋 사용)
    data(iris)

    # 데이터 분할 (훈련 세트와 테스트 세트)
    set.seed(123)
    train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
    train_data <- iris[train_index, ]
    test_data <- iris[-train_index, ]

    # 랜덤 포레스트 모델 생성
    rf_model <- randomForest(Species ~ ., data = train_data, ntree = 500, importance = TRUE)

    # 모델 요약
    print(rf_model)

    # 변수 중요도 시각화
    importance_df <- as.data.frame(importance(rf_model))
    importance_df$feature <- rownames(importance_df)

    ggplot(importance_df, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
      geom_bar(stat = "identity", fill = "skyblue") +
      coord_flip() +
      theme_minimal() +
      labs(title = "랜덤 포레스트 - 변수 중요도",
           x = "특성",
           y = "평균 지니 계수 감소")

    # 테스트 세트에 대한 예측
    predictions <- predict(rf_model, newdata = test_data)

    # 모델 평가 (정확도, 혼동 행렬)
    accuracy <- mean(predictions == test_data$Species)
    conf_matrix <- table(Predicted = predictions, Actual = test_data$Species)

    print(paste("정확도:", accuracy))
    print("혼동 행렬:")
    print(conf_matrix)

    # 첫 두 특성을 사용한 결정 경계 시각화
    rf_model2 <- randomForest(Species ~ Sepal.Length+Sepal.Width, data = train_data, ntree = 500, importance = TRUE)

    plot_data <- expand.grid(
      Sepal.Length = seq(min(iris$Sepal.Length), max(iris$Sepal.Length), length.out = 100),
      Sepal.Width = seq(min(iris$Sepal.Width), max(iris$Sepal.Width), length.out = 100)

    )
    plot_data$Species <- predict(rf_model2, newdata = plot_data)

    ggplot() +
      geom_tile(data = plot_data, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.3) +
      geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
      theme_minimal() +
      labs(title = "랜덤 포레스트 - 결정 경계",
           x = "꽃받침 길이(Sepal.Length)",
           y = "꽃받침 너비(Sepal.Width)")

    # 설명:
    # 1. randomForest 패키지를 사용하여 iris 데이터셋에 대한 분류 모델을 생성합니다.
    # 2. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
    # 3. randomForest() 함수로 모델을 생성합니다.
    # 4. 모델 요약 정보를 출력합니다.
    # 5. 변수 중요도를 시각화합니다.
    # 6. 테스트 세트에 대한 예측을 수행하고 정확도와 혼동 행렬을 계산하여 모델 성능을 평가합니다.
    # 7. 첫 두 특성(꽃받침 길이와 너비)을 사용하여 결정 경계를 시각화합니다.

    # 이 예제를 통해 랜덤 포레스트를 사용한 다중 분류의 과정을 이해할 수 있습니다.
    ```

3.  결정트리 (rpart : 의사결정나무)

```{r}
# 결정 트리 분류 예제
# 필요한 패키지 로드
library(rpart)
library(rpart.plot)
library(caret)
library(ggplot2)

# 데이터 준비 (iris 데이터셋 사용)
data(iris)

# 데이터 분할 (훈련 세트와 테스트 세트)
set.seed(123)
train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# 결정 트리 모델 생성
tree_model <- rpart(Species ~ ., data = train_data, method = "class")

# 모델 요약
summary(tree_model)

# 결정 트리 시각화
rpart.plot(tree_model, box.palette = "RdYlBu", shadow.col = "gray", nn = TRUE)

# 변수 중요도 시각화
    importance_df <- as.data.frame(importance(rf_model))
    importance_df$feature <- rownames(importance_df)

importance_df <- data.frame(
  feature = names(tree_model$variable.importance),
  importance = tree_model$variable.importance
)

ggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "결정 트리 - 변수 중요도",
       x = "특성",
       y = "중요도")

# 테스트 세트에 대한 예측
predictions <- predict(tree_model, newdata = test_data, type = "class")

# 모델 평가 (정확도, 혼동 행렬)
accuracy <- mean(predictions == test_data$Species)
conf_matrix <- table(Predicted = predictions, Actual = test_data$Species)

print(paste("정확도:", accuracy))
print("혼동 행렬:")
print(conf_matrix)



# 첫 두 특성을 사용한 결정 경계 시각화
tree_model2 <- rpart(Species ~Sepal.Length+Sepal.Width, data = train_data, method = "class")


plot_data <- expand.grid(
  Sepal.Length = seq(min(iris$Sepal.Length), max(iris$Sepal.Length), length.out = 100),
  Sepal.Width = seq(min(iris$Sepal.Width), max(iris$Sepal.Width), length.out = 100)
)
plot_data$Species <- predict(tree_model2, newdata = plot_data, type = "class")

ggplot() +
  geom_tile(data = plot_data, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.3) +
  geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  theme_minimal() +
  labs(title = "결정 트리 - 결정 경계",
       x = "꽃받침 길이",
       y = "꽃받침 너비")

# 설명:
# 1. rpart 패키지를 사용하여 iris 데이터셋에 대한 결정 트리 모델을 생성합니다.
# 2. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
# 3. rpart() 함수로 모델을 생성합니다. method="class"는 분류 트리를 의미합니다.
# 4. 모델 요약 정보를 출력합니다.
# 5. rpart.plot() 함수로 결정 트리를 시각화합니다.
# 6. 변수 중요도를 시각화합니다.
# 7. 테스트 세트에 대한 예측을 수행하고 정확도와 혼동 행렬을 계산하여 모델 성능을 평가합니다.
# 8. 첫 두 특성(꽃받침 길이와 너비)을 사용하여 결정 경계를 시각화합니다.

# 이 예제를 통해 결정 트리를 사용한 다중 분류의 과정을 이해할 수 있습니다.
```

4.  SVM(서포트 벡터 머신)

```{r}
# SVM 분류 예제
# 필요한 패키지 로드
library(e1071)
library(caret)
library(ggplot2)

# 데이터 준비 (iris 데이터셋 사용)
data(iris)

# 데이터 정규화 함수
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# 데이터 정규화
iris_normalized <- as.data.frame(lapply(iris[, 1:4], normalize))
iris_normalized$Species <- iris$Species

# 데이터 분할 (훈련 세트와 테스트 세트)
set.seed(123)
train_index <- createDataPartition(iris_normalized$Species, p = 0.7, list = FALSE)
train_data <- iris_normalized[train_index, ]
test_data <- iris_normalized[-train_index, ]

# SVM 모델 생성
svm_model <- svm(Species ~ ., data = train_data, kernel = "radial", cost = 1, gamma = 0.5)

# 모델 요약
summary(svm_model)

# 테스트 세트에 대한 예측
predictions <- predict(svm_model, newdata = test_data)

# 모델 평가 (정확도, 혼동 행렬)
accuracy <- mean(predictions == test_data$Species)
conf_matrix <- table(Predicted = predictions, Actual = test_data$Species)

print(paste("정확도:", accuracy))
print("혼동 행렬:")
print(conf_matrix)

# 첫 두 특성을 사용한 결정 경계 시각화

svm_model2 <- svm(Species ~Sepal.Length+Sepal.Width, data = train_data, kernel = "radial", cost = 1, gamma = 0.5)

plot_data <- expand.grid(
  Sepal.Length = seq(0, 1, length.out = 100),
  Sepal.Width = seq(0, 1, length.out = 100)
)
plot_data$Species <- predict(svm_model2, newdata = plot_data)

ggplot() +
  geom_tile(data = plot_data, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.3) +
  geom_point(data = iris_normalized, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  theme_minimal() +
  labs(title = "SVM - 결정 경계",
       x = "정규화된 꽃받침 길이",
       y = "정규화된 꽃받침 너비")

# 설명:
# 1. e1071 패키지를 사용하여 iris 데이터셋에 대한 SVM 모델을 생성합니다.
# 2. 데이터를 정규화합니다. SVM은 특성의 스케일에 민감하므로 이 단계가 중요합니다.
# 3. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
# 4. svm() 함수로 모델을 생성합니다. 여기서는 RBF 커널을 사용합니다.
# 5. 모델 요약 정보를 출력합니다.
# 6. 테스트 세트에 대한 예측을 수행하고 정확도와 혼동 행렬을 계산하여 모델 성능을 평가합니다.
# 7. 첫 두 특성(꽃받침 길이와 너비)을 사용하여 결정 경계를 시각화합니다.

# 이 예제를 통해 SVM을 사용한 다중 분류의 과정을 이해할 수 있습니다.
# 주의: SVM은 대규모 데이터셋에 대해 계산 비용이 높을 수 있습니다.
```

5.  KNN(K 최근접이웃 모델)

```{r}
# KNN 분류 예제
# 필요한 패키지 로드
library(class)
library(caret)
library(ggplot2)

# 데이터 준비 (iris 데이터셋 사용)
data(iris)

# 데이터 정규화 함수
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# 데이터 정규화
iris_normalized <- as.data.frame(lapply(iris[, 1:4], normalize))
iris_normalized$Species <- iris$Species

# 데이터 분할 (훈련 세트와 테스트 세트)
set.seed(123)
train_index <- createDataPartition(iris_normalized$Species, p = 0.7, list = FALSE)
train_data <- iris_normalized[train_index, ]
test_data <- iris_normalized[-train_index, ]

# KNN 모델 생성 및 예측 (k=5 사용)
k <- 5
predictions <- knn(train = train_data[, 1:4], 
                   test = test_data[, 1:4], 
                   cl = train_data$Species, 
                   k = k)

# 모델 평가 (정확도, 혼동 행렬)
accuracy <- mean(predictions == test_data$Species)
conf_matrix <- table(Predicted = predictions, Actual = test_data$Species)

print(paste("정확도:", accuracy))
print("혼동 행렬:")
print(conf_matrix)

# 첫 두 특성을 사용한 결정 경계 시각화


plot_data <- expand.grid(
  Sepal.Length = seq(0, 1, length.out = 100),
  Sepal.Width = seq(0, 1, length.out = 100)
)
plot_predictions <- knn(train = train_data[, 1:2], 
                        test = plot_data, 
                        cl = train_data$Species, 
                        k = k)

plot_data$Species <- plot_predictions

ggplot() +
  geom_tile(data = plot_data, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.3) +
  geom_point(data = iris_normalized, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  theme_minimal() +
  labs(title = paste("KNN (k =", k, ") - 결정 경계"),
       x = "정규화된 꽃받침 길이",
       y = "정규화된 꽃받침 너비")

# KNN의 k 값에 따른 정확도 변화
k_values <- 1:20
accuracies <- sapply(k_values, function(k) {
  predictions <- knn(train = train_data[, 1:4], 
                     test = test_data[, 1:4], 
                     cl = train_data$Species, 
                     k = k)
  mean(predictions == test_data$Species)
})

ggplot(data.frame(k = k_values, accuracy = accuracies), aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "KNN - k 값에 따른 정확도 변화",
       x = "k",
       y = "정확도")

# 설명:
# 1. class 패키지의 knn() 함수를 사용하여 iris 데이터셋에 대한 KNN 모델을 생성합니다.
# 2. 데이터를 정규화합니다. KNN은 특성의 스케일에 민감하므로 이 단계가 중요합니다.
# 3. 데이터를 훈련 세트와 테스트 세트로 분할합니다.
# 4. knn() 함수로 예측을 수행합니다. 여기서는 k=5를 사용합니다.
# 5. 정확도와 혼동 행렬을 계산하여 모델 성능을 평가합니다.
# 6. 첫 두 특성(꽃받침 길이와 너비)을 사용하여 결정 경계를 시각화합니다.
# 7. k 값에 따른 정확도 변화를 그래프로 나타냅니다.

# 이 예제를 통해 KNN을 사용한 다중 분류의 과정과 k 값 선택의 중요성을 이해할 수 있습니다.
```

### 2.3 모델평가

1.  교차 검증 (Cross-Validation) 교차 검증은 모델의 일반화 성능을 평가하기 위한 방법입니다. 가장 일반적인 형태는 k-fold 교차 검증입니다.

k-fold 교차 검증: 데이터를 k개의 부분집합으로 나누고, k번의 학습과 평가를 수행합니다. 각 반복에서 하나의 부분집합을 테스트 세트로, 나머지를 훈련 세트로 사용합니다.

2.  회귀 분석 평가 지표 R-squared (결정계수)

모델이 데이터의 변동성을 얼마나 잘 설명하는지 나타냅니다. 0에서 1 사이의 값을 가지며, 1에 가까울수록 모델의 설명력이 높습니다.

Mean Squared Error (MSE)

예측값과 실제값의 차이를 제곱하여 평균한 값입니다. 값이 작을수록 모델의 성능이 좋습니다.

Root Mean Squared Error (RMSE)

MSE의 제곱근입니다. 원래 데이터와 같은 단위를 가집니다.

3.  분류 분석 평가 지표 정확도 (Accuracy)

#### Confusion Matrix 관련 평가 지표

먼저, confusion matrix의 기본 구조를 살펴보겠습니다:

|                | 예측: Positive      | 예측: Negative      |
|----------------|---------------------|---------------------|
| 실제: Positive | TP (True Positive)  | FN (False Negative) |
| 실제: Negative | FP (False Positive) | TN (True Negative)  |

이 matrix를 바탕으로 다양한 평가 지표를 계산할 수 있습니다:

| 지표                                      | 수식                                                                    | 설명                                         |
|------------------------|------------------------|------------------------|
| 정확도 (Accuracy)                         | $\frac{TP + TN}{TP + TN + FP + FN}$                                     | 전체 예측 중 올바른 예측의 비율              |
| 정밀도 (Precision)                        | $\frac{TP}{TP + FP}$                                                    | Positive로 예측한 것 중 실제 Positive의 비율 |
| 재현율 (Recall) 또는 민감도 (Sensitivity) | $\frac{TP}{TP + FN}$                                                    | 실제 Positive 중 Positive로 예측한 비율      |
| 특이도 (Specificity)                      | $\frac{TN}{TN + FP}$                                                    | 실제 Negative 중 Negative로 예측한 비율      |
| F1 점수                                   | $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$             | 정밀도와 재현율의 조화평균                   |
| MCC (Matthews Correlation Coefficient)    | $\frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$ | -1에서 1 사이의 값, 1에 가까울수록 좋은 예측 |

추가적인 지표들:

| 지표                            | 수식                 | 설명                                         |
|------------------------|------------------------|------------------------|
| False Positive Rate (FPR)       | $\frac{FP}{FP + TN}$ | 실제 Negative 중 Positive로 잘못 예측한 비율 |
| False Negative Rate (FNR)       | $\frac{FN}{TP + FN}$ | 실제 Positive 중 Negative로 잘못 예측한 비율 |
| Positive Predictive Value (PPV) | $\frac{TP}{TP + FP}$ | 정밀도와 동일                                |
| Negative Predictive Value (NPV) | $\frac{TN}{TN + FN}$ | Negative로 예측한 것 중 실제 Negative의 비율 |

이러한 지표들은 모델의 성능을 다양한 각도에서 평가할 수 있게 해줍니다. 문제의 특성과 중요도에 따라 적절한 지표를 선택하여 사용해야 합니다.

AUC-ROC

ROC 곡선 아래의 면적입니다. 모델의 분류 성능을 나타내며, 0.5에서 1 사이의 값을 가집니다. 1에 가까울수록 좋은 성능을 의미합니다.

```{r}
# Confusion Matrix 시각화
library(caret)
library(ggplot2)
library(randomForest)

# 데이터 준비 (iris 데이터셋 사용)
data(iris)
iris_binary <- iris[iris$Species != "virginica", ]
iris_binary$Species <- factor(iris_binary$Species)

# 모델 훈련
set.seed(123)
rf_model <- randomForest(Species ~ ., data = iris_binary)

# 예측
predictions <- predict(rf_model, iris_binary)

# Confusion Matrix 생성
cm <- confusionMatrix(predictions, iris_binary$Species)

# Confusion Matrix 시각화
cm_d <- as.data.frame(cm$table)
cm_d$Prediction <- factor(cm_d$Prediction, levels = rev(levels(cm_d$Prediction)))

ggplot(cm_d, aes(Prediction, Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 0.5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(title = "Confusion Matrix") +
  coord_fixed()

# 평가 지표 시각화
metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1", "Specificity"),
  Value = c(cm$overall['Accuracy'], 
            cm$byClass['Precision'],
            cm$byClass['Recall'],
            cm$byClass['F1'],
            cm$byClass['Specificity'])
)

ggplot(metrics, aes(x = Metric, y = Value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = sprintf("%.3f", Value)), vjust = -0.3) +
  theme_minimal() +
  labs(title = "Model Performance Metrics") +
  ylim(0, 1)

# 설명:
# 1. randomForest 모델을 훈련시키고 예측을 수행합니다.
# 2. confusionMatrix 함수를 사용하여 confusion matrix와 관련 지표를 계산합니다.
# 3. ggplot2를 사용하여 confusion matrix를 히트맵 형태로 시각화합니다.
# 4. 주요 평가 지표들을 막대 그래프로 시각화합니다.

# 이 코드를 실행하면 confusion matrix와 주요 평가 지표들을 시각적으로 이해하기 쉽게 표현할 수 있습니다.
```

```{r}
# 로지스틱 회귀 예제 (평가 지표 포함)
library(caret)
library(pROC)
library(ggplot2)

# 데이터 준비 (iris 데이터셋 사용)
data(iris)
# 간단한 이진 분류를 위해 setosa와 versicolor만 사용
iris_binary <- iris[iris$Species != "virginica", ]
iris_binary$Species <- factor(iris_binary$Species)

# 교차 검증 설정
ctrl <- trainControl(method = "cv", number = 5, 
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

# 로지스틱 회귀 모델 생성 및 교차 검증
model <- train(Species ~ Petal.Length + Petal.Width, 
               data = iris_binary, 
               method = "glm", 
               family = "binomial",
               trControl = ctrl,
               metric = "ROC")

# 모델 요약
print(model)

# 평가 지표
print(model$results)

# 혼동 행렬
confusionMatrix(model)

# ROC 곡선
roc_obj <- roc(iris_binary$Species, predict(model, iris_binary, type = "prob")[, "versicolor"])
plot(roc_obj, main = paste("ROC Curve (AUC =", round(auc(roc_obj), 3), ")"))

# 결정 경계 시각화
ggplot(iris_binary, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() +
  stat_function(fun = function(x) {-coef(model$finalModel)[1]/coef(model$finalModel)[3] - coef(model$finalModel)[2]/coef(model$finalModel)[3] * x}, 
                geom = "line", color = "black") +
  theme_minimal() +
  labs(title = paste("로지스틱 회귀 - 결정 경계 (AUC =", round(auc(roc_obj), 3), ")"),
       x = "꽃잎 길이",
       y = "꽃잎 너비")

# 설명:
# 1. caret 패키지를 사용하여 5-fold 교차 검증을 수행합니다.
# 2. twoClassSummary 함수를 사용하여 이진 분류에 적합한 평가 지표를 계산합니다.
# 3. train() 함수로 모델을 생성하고 교차 검증을 수행합니다.
# 4. 모델 요약과 평가 지표(ROC, Sensitivity, Specificity)를 출력합니다.
# 5. 혼동 행렬을 생성하여 모델의 성능을 자세히 살펴봅니다.
# 6. ROC 곡선을 그리고 AUC를 계산합니다.
# 7. 결정 경계를 시각화하고 AUC 값을 포함합니다.

# 이 예제를 통해 로지스틱 회귀 모델의 성능을 다양한 지표로 평가할 수 있습니다.
```

### 

### 2.4 tidymodel 패키지를 이용한 머신러닝

\[tidymodels 패키지\] <https://www.tidymodels.org/>

```{mermaid}
flowchart TD
    A[`데이터 준비`] --> B[`데이터 분할`]
    B --> C[`레시피 정의`]
    B --> D[`모델 명세`]
    C --> E[`워크플로우 정의`]
    D --> E
    E --> F[`모델 학습`]
    F --> G[`모델 평가`]
    G --> H[`성능 시각화`]
    
    A -.-> [tidyverse] B
    B -.-> [rsample] C
    B -.-> [parsnip] D
    C -.-> [recipes] E
    D -.-> [parsnip] E
    E -.-> [workflows] F
    F -.-> [yardstick] G
    G -.-> [ggplot2] H
    

    
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:1px;
    classDef highlight fill:#e6f3ff,stroke:#333,stroke-width:2px;
    classDef package fill:#f9f9f9,stroke:#999,stroke-width:1px,color:#666;
    
    class A,B,C,D,E,F,G,H highlight;
    class tidyverse,rsample,parsnip,recipes,workflows,yardstick,ggplot2 package;
    
    linkStyle default stroke:#666,stroke-width:1px;
```

```{r}
# tidymodels를 이용한 회귀 및 분류 분석 예제
# 필요한 패키지 설치 및 로드
#install.packages("tidymodels")
library(tidymodels)

# 1. 회귀 분석 예제 (mtcars 데이터셋 사용)

# 데이터 준비
data(mtcars)
set.seed(123)
car_split <- initial_split(mtcars, prop = 0.7, strata = mpg)
car_train <- training(car_split)
car_test <- testing(car_split)

# 모델 정의 (선형 회귀)
lm_model <- linear_reg() %>% 
  set_engine("lm")

# 레시피 정의
car_recipe <- recipe(mpg ~ ., data = car_train) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors())

# 워크플로우 정의
lm_workflow <- workflow() %>%
  add_model(lm_model) %>%
  add_recipe(car_recipe)

# 모델 학습
lm_fit <- fit(lm_workflow, data = car_train)

# 예측 및 평가
car_predictions <- predict(lm_fit, new_data = car_test) %>%
  bind_cols(car_test)

# 평가 지표 계산
car_metrics <- car_predictions %>%
  metrics(truth = mpg, estimate = .pred)

print("회귀 분석 결과:")
print(car_metrics)

# 2. 분류 분석 예제 (iris 데이터셋 사용)

# 데이터 준비
data(iris)
set.seed(123)
iris_split <- initial_split(iris, prop = 0.7, strata = Species)
iris_train <- training(iris_split)
iris_test <- testing(iris_split)

# 모델 정의 (랜덤 포레스트)
rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

# 레시피 정의
iris_recipe <- recipe(Species ~ ., data = iris_train) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors())

# 워크플로우 정의
rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(iris_recipe)

# 모델 학습
rf_fit <- fit(rf_workflow, data = iris_train)

# 예측 및 평가
iris_predictions <- predict(rf_fit, new_data = iris_test) %>%
  bind_cols(predict(rf_fit, new_data = iris_test, type = "prob")) %>%
  bind_cols(iris_test)

# 평가 지표 계산
iris_metrics <- iris_predictions %>%
  metrics(truth = Species, estimate = .pred_class)

print("분류 분석 결과:")
print(iris_metrics)

# ROC 커브 그리기
iris_roc <- iris_predictions %>%
  roc_curve(Species, .pred_setosa:.pred_virginica) %>%
  autoplot()

print(iris_roc)

# 설명:
# 1. 회귀 분석:
#    - mtcars 데이터셋을 사용하여 mpg를 예측하는 선형 회귀 모델을 만듭니다.
#    - initial_split()으로 데이터를 훈련셋과 테스트셋으로 나눕니다.
#    - recipe()로 전처리 과정을 정의합니다.
#    - workflow()로 모델과 전처리 과정을 결합합니다.
#    - fit()으로 모델을 학습시키고, predict()로 예측을 수행합니다.
#    - metrics()로 RMSE, R-squared 등의 평가 지표를 계산합니다.

# 2. 분류 분석:
#    - iris 데이터셋을 사용하여 Species를 분류하는 랜덤 포레스트 모델을 만듭니다.
#    - 회귀 분석과 유사한 과정을 거치지만, set_mode("classification")으로 분류 모드를 설정합니다.
#    - metrics()로 정확도, AUC 등의 평가 지표를 계산합니다.
#    - roc_curve()와 autoplot()으로 ROC 커브를 그립니다.

# 이 예제를 통해 tidymodels를 사용하여 일관된 방식으로 회귀와 분류 분석을 수행할 수 있음을 보여줍니다.
```
